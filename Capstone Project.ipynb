{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#import all necessary files for capstone project\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from io import StringIO\n",
    "import configparser\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import datetime\n",
    "from sqlalchemy import create_engine\n",
    "from sql_queries import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DateType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, rand\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql.functions import expr,from_unixtime,row_number,dayofweek,year,month,dayofmonth,hour,date_format,desc,col,dense_rank,rank,weekofyear,monotonically_increasing_id\n",
    "from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Dbl, StringType as Str, IntegerType as Int, DateType as Date, TimestampType\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Point to the dwh.cfg file to get the access and secret keys for reading and writing to S3 on AWS\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dwh.cfg')\n",
    "\n",
    "# Read the AWS access and secret keys\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config.get('AWS','KEY')\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config.get('AWS','SECRET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#define measurer to get max len for each column to give datatype length in model\n",
    "measurer = np.vectorize(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Create S3 bucket connection\"\"\"\n",
    "s3 = boto3.resource('s3',region_name='us-west-2',aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\\\n",
    "                    aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Build spark session\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "### Read immigration data for april in SAS format\n",
    "df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "#cast ARRIVAL_DATE column to timestamp\n",
    "get_timestamp = udf(lambda x: x/1000, Dbl())\n",
    "df_spark = df_spark.withColumn('ts2', get_timestamp('arrdate'))\n",
    "df_spark = df_spark.withColumn('ARRIVAL_DATE', from_unixtime('ts2').cast(dataType=TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# rename and cast the columns in the immigration dataframe\n",
    "df_spark1 = df_spark.selectExpr(\"cast(cicid as integer)  IMMIGRANT_ID\",\"cast(i94yr as int) ARRIVAL_YEAR\", \"cast(i94mon as int) ARRIVAL_MONTH\", \"cast(i94cit as int) COUNTY_CITIZEN\", \\\n",
    "                         \"cast(i94res as int)COUNTY_RESIDENCE\", \"cast(i94port as varchar(10)) PORT_ID\",\"cast(ARRIVAL_DATE as date) ARRIVAL_DATE\", \"cast(i94mode as int) MODE_ID\",\\\n",
    "                        \"i94addr as STATE_ID\",\"cast(year(current_date())-biryear as int) AGE\", \"cast(i94visa as int) VISA_ID\", \"matflag as MATCH_FLAG\", \"cast(biryear as int) BIRTH_YEAR\", \"gender as GENDER\",\\\n",
    "                         \"airline as AIRLINE\", \"cast(admnum as bigint) ADMISSION_NUMBER\", \"fltno as FLIGHT_NO\", \"visatype as VISA_TYPE\"\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#check the dataframe structure (quality check)\n",
    "df_spark1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#buffer dataframe to csv on S3 bucket\n",
    "df_spark1=df_spark1.repartition(1)\n",
    "#save CSV tolocal location and replace \"\" with null values(quality check)\n",
    "df_spark1.write.format('csv').option('header',True).mode('overwrite'). option('sep','|').save(\"IMM_FILES/\",nullValue=None)\n",
    "#Define S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "#get CSV file\n",
    "csv_files = glob.glob(\"IMM_FILES/*.csv\")\n",
    "#upload csvto S3 bucket , named it IMMIGRATION and set null values to nothing\n",
    "for filename in csv_files:\n",
    "    s3_client.upload_file(filename,\"capstone-kobap\",'IMMIGRATION.CSV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#named csv_buffer as StringIO to write files to S3\n",
    "csv_buffer = StringIO()\n",
    "#read airport-codes csv file in dataframe \n",
    "df_airport=pd.read_csv('airport-codes_csv.csv')\n",
    "#rename columns to meet target table\n",
    "df_airport=df_airport.rename(columns={\"type\": \"AIRPORT_TYPE\", \"name\": \"AIRPORT_NAME\", \"iso_country\": \"COUNTRY\"})\n",
    "df_airport['STATE_ID'] = df_airport['iso_region'].str[3:]\n",
    "df_airport.to_csv(csv_buffer,sep='|',header='True', index=False)\n",
    "#buffer dataframe to csv on S3 bucket\n",
    "s3.Object('capstone-kobap', 'airport.CSV').put(Body=csv_buffer.getvalue())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#check dataframe info\n",
    "df_airport.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check dataframe count\n",
    "df_airport.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#check dataframe describtion\n",
    "df_airport.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#get the max lenght for eich column in airport dataframe \n",
    "res1 = measurer(df_airport.values.astype(str)).max(axis=0)\n",
    "res1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# reading I94_SAS_Labels_Descriptions and cleansing it to newobject\n",
    "ha_list=[]\n",
    "with open('I94_SAS_Labels_Descriptions.SAS') as reader,open('newfile.txt', 'w') as newfile:\n",
    "    desc_lines = reader.readlines()\n",
    "    for line in desc_lines:\n",
    "        #new_string = line.replace(\",\", \"||\")\n",
    "        new_string = line.replace(\"\\n\", '')\n",
    "        new_string = new_string.replace(\"\\t\", '')\n",
    "        new_string = new_string.replace(\";\", '')\n",
    "        new_string = new_string.replace(\"'\", '')\n",
    "        ha_list.append(new_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#named csv_buffer as StringIO to write files to S3\n",
    "csv_buffer= StringIO()\n",
    "#get lines that contains city codes and description to dataframe\n",
    "df=pd.DataFrame(ha_list[9:298])\n",
    "Country_df=pd.DataFrame(df[0])\n",
    "#rename column to cities_ds\n",
    "Country_df.columns=['COUNTRY_ds']\n",
    "#add new columns splited from cities_ds to 'COUNTRY_ID','COUNTRY_DESC' delimeter '='\n",
    "Country_df[['COUNTRY_ID','COUNTRY_DESC']] = Country_df.COUNTRY_ds.str.split(\"=\",expand=True)\n",
    "#drop cities_ds column\n",
    "Country_df=Country_df.drop(['COUNTRY_ds'], axis=1)\n",
    "#write data to s3 bucket and named file to COUNTRY.csv\n",
    "Country_df.to_csv(csv_buffer,sep='|',header='True', index=False)\n",
    "s3.Object('capstone-kobap', 'COUNTRY.CSV').put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#gey country dataframe datatypes and structure\n",
    "Country_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#named csv_buffer as StringIO to write files to S3\n",
    "csv_buffer= StringIO()\n",
    "#get lines that contains port codes and description to dataframe\n",
    "df=pd.DataFrame(ha_list[302:962])\n",
    "Port_df=pd.DataFrame(df[0])\n",
    "#rename column to Port_ds\n",
    "Port_df.columns=['Port_ds']\n",
    "#add new columns splited from cities_ds to 'Port_id','Port_Desc' delimeter '='\n",
    "Port_df[['Port_id','Port_Desc']] = Port_df.Port_ds.str.split(\"=\",expand=True)\n",
    "#drop Port_ds column\n",
    "Port_df=Port_df.drop(['Port_ds'], axis=1)\n",
    "#buffer dataframe to csv on S3 bucket\n",
    "Port_df.to_csv(csv_buffer,sep='|',header='True', index=False)\n",
    "s3.Object('capstone-kobap', 'PORT.CSV').put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#named csv_buffer as StringIO to write files to S3\n",
    "csv_buffer= StringIO()\n",
    "#get lines that contains mode codes and description to dataframe\n",
    "df=pd.DataFrame(ha_list[972:976])\n",
    "model_df=pd.DataFrame(df)\n",
    "#rename column to model_ds\n",
    "model_df.columns=['model_ds']\n",
    "#trim model_ds column\n",
    "model_df['model_ds']= model_df['model_ds'].str.strip()\n",
    "#add new columns splited from model_ds to 'Model_Code','Model_Desc' delimeter '='\n",
    "model_df[['Model_Code','Model_Desc']] = model_df.model_ds.str.split(\"=\",expand=True)\n",
    "#drop model_ds column\n",
    "model_df=model_df.drop(['model_ds'], axis=1)\n",
    "#buffer dataframe to csv on S3 bucket\n",
    "model_df.to_csv(csv_buffer,sep='|',header='True', index=False)\n",
    "s3.Object('capstone-kobap', 'Model.CSV').put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#named csv_buffer as StringIO to write files to S3\n",
    "csv_buffer= StringIO()\n",
    "#get lines that contains address codes and description to dataframe\n",
    "df=pd.DataFrame(ha_list[981:1036])\n",
    "addrl_df=pd.DataFrame(df)\n",
    "#rename column to addrl_ds\n",
    "addrl_df.columns=['addrl_ds']\n",
    "#trim addrl_ds column\n",
    "addrl_df['addrl_ds']= addrl_df['addrl_ds'].str.strip()\n",
    "#add new columns splited from addrl_ds to 'STATE_ID','STATE' delimeter '='\n",
    "addrl_df[['STATE_ID','STATE']] = addrl_df.addrl_ds.str.split(\"=\",expand=True)\n",
    "#drop addrl_ds column\n",
    "addrl_df=addrl_df.drop(['addrl_ds'], axis=1)\n",
    "#buffer dataframe to csv on S3 bucket\n",
    "addrl_df.to_csv(csv_buffer,sep='|',header='True', index=False)\n",
    "s3.Object('capstone-kobap', 'STATE.CSV').put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#named csv_buffer as StringIO to write files to S3\n",
    "csv_buffer= StringIO()\n",
    "#get lines that contains VISA codes and description to dataframe\n",
    "df=pd.DataFrame(ha_list[1046:1049])\n",
    "VISA_df=pd.DataFrame(df)\n",
    "#rename column to VISA_ds\n",
    "VISA_df.columns=['VISA_ds']\n",
    "#trim VISA_ds column\n",
    "VISA_df['VISA_ds']= VISA_df['VISA_ds'].str.strip()\n",
    "#add new columns splited from VISA_ds to 'VISA_Code','VISA_Desc' delimeter '='\n",
    "VISA_df[['VISA_Code','VISA_Desc']] = VISA_df.VISA_ds.str.split(\"=\",expand=True)\n",
    "#drop VISA_ds column\n",
    "VISA_df=VISA_df.drop(['VISA_ds'], axis=1)\n",
    "#buffer dataframe to csv on S3 bucket\n",
    "VISA_df.to_csv(csv_buffer,sep='|',header='True', index=False)\n",
    "s3.Object('capstone-kobap', 'VISA.CSV').put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#named csv_buffer as StringIO to write files to S3\n",
    "csv_buffer= StringIO()\n",
    "#read demographics csv file to dataframe \n",
    "DEMOGRAPHICS_df=pd.read_csv('us-cities-demographics.csv',delimiter=';')\n",
    "#rename columns to meet target table\n",
    "DEMOGRAPHICS_df=DEMOGRAPHICS_df.rename(columns={\"Median Age\": \"MEDIAN_AGE\", \"Male Population\": \"MALE_POPULATION\", \"Female Population\": \"FEMALE_POPULATION\", \"Total Population\": \"TOTAL_POPULATION\"})\n",
    "DEMOGRAPHICS_df=DEMOGRAPHICS_df.rename(columns={\"Foreign-born\": \"FOREIGN_BORN\", \"Average Household Size\": \"AVE_HOUSEHOLD\", \"State Code\": \"STATE_ID\", \"Count\": \"POP_COUNT\"})\n",
    "#drop not needed columns\n",
    "DEMOGRAPHICS_df=DEMOGRAPHICS_df.drop(['Number of Veterans'], axis=1)\n",
    "#buffer dataframe to csv on S3 bucket\n",
    "DEMOGRAPHICS_df.to_csv(csv_buffer,sep='|',header='True', index=False)\n",
    "s3.Object('capstone-kobap', 'DEMOGRAPHICS.CSV').put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#--------------------------All data buffered to s3 bucket completly------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#--------------------------preparing moving data to redshift------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#get required configration from dwh.cfg\n",
    "\"\"\" Load DWH Params from a file\"\"\"\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')\n",
    "\n",
    "DWH_CLUSTER_TYPE       = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES          = config.get(\"DWH\",\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE          = config.get(\"DWH\",\"DWH_NODE_TYPE\")\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB                 = config.get(\"DWH\",\"DWH_DB\")\n",
    "DWH_DB_USER            = config.get(\"DWH\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD        = config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT               = config.get(\"DWH\",\"DWH_PORT\")\n",
    "\n",
    "DWH_IAM_ROLE_NAME      = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#define iam and redshift client\n",
    "iam = boto3.client('iam',region_name='us-west-2',aws_access_key_id=KEY,aws_secret_access_key=SECRET)\n",
    "redshift =boto3.client('redshift',region_name='us-west-2',aws_access_key_id=KEY, aws_secret_access_key=SECRET) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# print('1.2 Attaching Policy')\n",
    "iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
    "                        PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                       )['ResponseMetadata']['HTTPStatusCode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Create an IAM Role that makes Redshift able to access S3 bucket (ReadOnly)\n",
    "Create the iam role if not existed\"\"\"\n",
    "try:\n",
    "    print('1.1 Creating a new IAM Role')\n",
    "    dwhRole =iam.create_role(\n",
    "    Path='/',\n",
    "    RoleName=DWH_IAM_ROLE_NAME,\n",
    "    Description=\"allow redshift to call aws services\",\n",
    "    AssumeRolePolicyDocument=json.dumps(\n",
    "    {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "    'Effect': 'Allow',\n",
    "    'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "    'Version': '2012-10-17'}\n",
    "    )\n",
    "\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "     print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# print('1.3 Get the IAM role ARN')\n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn'] \n",
    "roleArn\n",
    "# print(roleArn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#create redshift cluster if not existed \n",
    "try:\n",
    "    response = redshift.create_cluster(        \n",
    "             # TODO: add parameters for hardware\n",
    "    ClusterType=DWH_CLUSTER_TYPE,\n",
    "    NodeType=DWH_NODE_TYPE,\n",
    "    NumberOfNodes=int(DWH_NUM_NODES),\n",
    "\n",
    "             # TODO: add parameters for identifiers & credentials\n",
    "    DBName=DWH_DB,\n",
    "    ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "    MasterUsername=DWH_DB_USER,\n",
    "    MasterUserPassword=DWH_DB_PASSWORD,\n",
    "\n",
    "\n",
    "             # TODO: add parameter for role (to allow s3 access)\n",
    "    IamRoles=[roleArn]\n",
    "     )\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#get redshift properties \n",
    "def prettyRedshiftProps(props):\n",
    "    \"\"\"get redshift below properties to prepare database connection and check cluster status\"\"\"\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#define Endpoint and role_arn \n",
    "DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#initiate redshift database connection \n",
    "conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(DWH_ENDPOINT,DWH_DB,DWH_DB_USER,DWH_DB_PASSWORD,DWH_PORT))\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Creation of capstone schema\"\"\"\n",
    "#create and set schema\n",
    "cur.execute(CREATE_SCHEMA)\n",
    "conn.commit()\n",
    "cur.execute(SET_SCHEMA)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"drop of fact and Dimensions Tables\"\"\"\n",
    "#drop all tables if exsited \n",
    "for query in drop_table_queries:\n",
    "    #print(query)\n",
    "    cur.execute(query)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Creation of fact and Dimensions Tables\"\"\"\n",
    "#create tables\n",
    "for query in create_table_queries:\n",
    "    cur.execute(query)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#load data to target tables\n",
    "for query in copy_table_queries:\n",
    "    cur.execute(query)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#create numeric list contains fact and  dimension tables by looping on count_table_queries\n",
    "count_list = []\n",
    "for query in count_table_queries:\n",
    "    cur.execute(query)\n",
    "    tbl_count=cur.fetchall()\n",
    "    res=tbl_count[0]\n",
    "    count_list.append(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#check data warehouse tables count with dataframes\n",
    "print('immigrants dataframe=', df_spark1.count(),' inserted records= ',count_list[0])\n",
    "print('AIRPORT dataframe=', len(df_airport),' inserted records= ',count_list[4])\n",
    "print('Country dataframe=', len(Country_df),' inserted records= ',count_list[7])\n",
    "print('PORT dataframe=', len(Port_df),' inserted records= ',count_list[2])\n",
    "print('MODE dataframe=', len(model_df),' inserted records= ',count_list[6])\n",
    "print('DEMOGRAPHICS dataframe=', len(DEMOGRAPHICS_df),' inserted records= ',count_list[1])\n",
    "print('STATE dataframe=', len(addrl_df),' inserted records= ',count_list[5])\n",
    "print('VISA dataframe=', len(VISA_df),' inserted records= ',count_list[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Quality Checks#\n",
    "#1- trim values\n",
    "#2- relace \"\" with none\n",
    "#3- Check count between inserted records and dataframes\n",
    "#4- analysis quality checks to can understand the data to implement the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Delete Redshift Cluster\n",
    "redshift.delete_cluster( ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,  SkipFinalClusterSnapshot=True)\n",
    "iam.detach_role_policy(RoleName=DWH_IAM_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "iam.delete_role(RoleName=DWH_IAM_ROLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
